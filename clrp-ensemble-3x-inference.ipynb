{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "therapeutic-monthly",
   "metadata": {
    "papermill": {
     "duration": 0.042963,
     "end_time": "2021-08-01T07:46:26.244260",
     "exception": false,
     "start_time": "2021-08-01T07:46:26.201297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook combines three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complimentary-advance",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:26.350656Z",
     "iopub.status.busy": "2021-08-01T07:46:26.349886Z",
     "iopub.status.idle": "2021-08-01T07:46:29.425997Z",
     "shell.execute_reply": "2021-08-01T07:46:29.424978Z",
     "shell.execute_reply.started": "2021-08-01T06:29:41.881679Z"
    },
    "papermill": {
     "duration": 3.131673,
     "end_time": "2021-08-01T07:46:29.426172",
     "exception": false,
     "start_time": "2021-08-01T07:46:26.294499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "humanitarian-flooring",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:29.537323Z",
     "iopub.status.busy": "2021-08-01T07:46:29.536374Z",
     "iopub.status.idle": "2021-08-01T07:46:29.540216Z",
     "shell.execute_reply": "2021-08-01T07:46:29.540652Z",
     "shell.execute_reply.started": "2021-07-26T12:00:13.970499Z"
    },
    "papermill": {
     "duration": 0.083223,
     "end_time": "2021-08-01T07:46:29.540823",
     "exception": false,
     "start_time": "2021-08-01T07:46:29.457600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 248\n",
    "EVAL_SCHEDULE = [(0.5, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1, 1)]\n",
    "ROBERTA_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\n",
    "TOKENIZER_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "antique-falls",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:29.612286Z",
     "iopub.status.busy": "2021-08-01T07:46:29.611685Z",
     "iopub.status.idle": "2021-08-01T07:46:29.628581Z",
     "shell.execute_reply": "2021-08-01T07:46:29.628046Z",
     "shell.execute_reply.started": "2021-07-26T12:00:14.044654Z"
    },
    "papermill": {
     "duration": 0.055174,
     "end_time": "2021-08-01T07:46:29.628700",
     "exception": false,
     "start_time": "2021-08-01T07:46:29.573526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incorrect-sight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:29.698030Z",
     "iopub.status.busy": "2021-08-01T07:46:29.697283Z",
     "iopub.status.idle": "2021-08-01T07:46:29.940024Z",
     "shell.execute_reply": "2021-08-01T07:46:29.939047Z",
     "shell.execute_reply.started": "2021-07-26T12:00:14.073624Z"
    },
    "papermill": {
     "duration": 0.279365,
     "end_time": "2021-08-01T07:46:29.940169",
     "exception": false,
     "start_time": "2021-08-01T07:46:29.660804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-lloyd",
   "metadata": {
    "papermill": {
     "duration": 0.032914,
     "end_time": "2021-08-01T07:46:30.004989",
     "exception": false,
     "start_time": "2021-08-01T07:46:29.972075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "copyrighted-tucson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:30.080748Z",
     "iopub.status.busy": "2021-08-01T07:46:30.080074Z",
     "iopub.status.idle": "2021-08-01T07:46:30.083530Z",
     "shell.execute_reply": "2021-08-01T07:46:30.083090Z",
     "shell.execute_reply.started": "2021-07-26T12:00:14.324031Z"
    },
    "papermill": {
     "duration": 0.044941,
     "end_time": "2021-08-01T07:46:30.083672",
     "exception": false,
     "start_time": "2021-08-01T07:46:30.038731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-contact",
   "metadata": {
    "papermill": {
     "duration": 0.031806,
     "end_time": "2021-08-01T07:46:30.147723",
     "exception": false,
     "start_time": "2021-08-01T07:46:30.115917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1\n",
    "Inspired from: https://www.kaggle.com/maunish/clrp-roberta-svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "steady-crown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:30.220534Z",
     "iopub.status.busy": "2021-08-01T07:46:30.219849Z",
     "iopub.status.idle": "2021-08-01T07:46:30.223103Z",
     "shell.execute_reply": "2021-08-01T07:46:30.222677Z",
     "shell.execute_reply.started": "2021-07-26T12:00:14.333998Z"
    },
    "papermill": {
     "duration": 0.0435,
     "end_time": "2021-08-01T07:46:30.223219",
     "exception": false,
     "start_time": "2021-08-01T07:46:30.179719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.25,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "placed-polyester",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:30.293079Z",
     "iopub.status.busy": "2021-08-01T07:46:30.292407Z",
     "iopub.status.idle": "2021-08-01T07:46:30.296306Z",
     "shell.execute_reply": "2021-08-01T07:46:30.295841Z",
     "shell.execute_reply.started": "2021-07-26T12:00:14.347059Z"
    },
    "papermill": {
     "duration": 0.041706,
     "end_time": "2021-08-01T07:46:30.296418",
     "exception": false,
     "start_time": "2021-08-01T07:46:30.254712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-guitar",
   "metadata": {
    "papermill": {
     "duration": 0.031846,
     "end_time": "2021-08-01T07:46:30.359808",
     "exception": false,
     "start_time": "2021-08-01T07:46:30.327962",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expired-cradle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:46:30.436546Z",
     "iopub.status.busy": "2021-08-01T07:46:30.435566Z",
     "iopub.status.idle": "2021-08-01T07:47:20.902367Z",
     "shell.execute_reply": "2021-08-01T07:47:20.903068Z",
     "shell.execute_reply.started": "2021-07-26T12:00:14.362721Z"
    },
    "papermill": {
     "duration": 50.511376,
     "end_time": "2021-08-01T07:47:20.903270",
     "exception": false,
     "start_time": "2021-08-01T07:46:30.391894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:18<01:12, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:26<00:38, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:34<00:21, 10.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:42<00:09,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:50<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in tqdm(range(NUM_MODELS)):            \n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "published-substitute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:47:20.978648Z",
     "iopub.status.busy": "2021-08-01T07:47:20.977209Z",
     "iopub.status.idle": "2021-08-01T07:47:20.979591Z",
     "shell.execute_reply": "2021-08-01T07:47:20.980022Z",
     "shell.execute_reply.started": "2021-07-26T12:01:07.35903Z"
    },
    "papermill": {
     "duration": 0.041741,
     "end_time": "2021-08-01T07:47:20.980159",
     "exception": false,
     "start_time": "2021-08-01T07:47:20.938418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bearing-eagle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:47:21.059361Z",
     "iopub.status.busy": "2021-08-01T07:47:21.058773Z",
     "iopub.status.idle": "2021-08-01T07:48:00.631238Z",
     "shell.execute_reply": "2021-08-01T07:48:00.630732Z",
     "shell.execute_reply.started": "2021-07-26T12:01:07.365376Z"
    },
    "papermill": {
     "duration": 39.615739,
     "end_time": "2021-08-01T07:48:00.631376",
     "exception": false,
     "start_time": "2021-08-01T07:47:21.015637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/pre-trained-roberta-solution-in-pytorch/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:10<00:43, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/pre-trained-roberta-solution-in-pytorch/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:17<00:25,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/pre-trained-roberta-solution-in-pytorch/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:15,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/pre-trained-roberta-solution-in-pytorch/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:31<00:07,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/pre-trained-roberta-solution-in-pytorch/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:39<00:00,  7.89s/it]\n"
     ]
    }
   ],
   "source": [
    "ROBERTA_PATH = \"../input/pre-trained-roberta-solution-in-pytorch\"\n",
    "TOKENIZER_PATH = \"../input/pre-trained-roberta-solution-in-pytorch\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in tqdm(range(NUM_MODELS)):            \n",
    "    model_path = f\"../input/pre-trained-roberta-solution-in-pytorch/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "digital-chess",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:00.722938Z",
     "iopub.status.busy": "2021-08-01T07:48:00.722253Z",
     "iopub.status.idle": "2021-08-01T07:48:00.726881Z",
     "shell.execute_reply": "2021-08-01T07:48:00.726372Z",
     "shell.execute_reply.started": "2021-07-26T12:01:48.720791Z"
    },
    "papermill": {
     "duration": 0.052127,
     "end_time": "2021-08-01T07:48:00.727013",
     "exception": false,
     "start_time": "2021-08-01T07:48:00.674886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mdp = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-diagram",
   "metadata": {
    "papermill": {
     "duration": 0.043396,
     "end_time": "2021-08-01T07:48:00.813110",
     "exception": false,
     "start_time": "2021-08-01T07:48:00.769714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2\n",
    "Inspired from: [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-membrane",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:00.906867Z",
     "iopub.status.busy": "2021-08-01T07:48:00.906274Z",
     "iopub.status.idle": "2021-08-01T07:48:05.727859Z",
     "shell.execute_reply": "2021-08-01T07:48:05.726901Z",
     "shell.execute_reply.started": "2021-07-26T12:01:48.72934Z"
    },
    "papermill": {
     "duration": 4.873497,
     "end_time": "2021-08-01T07:48:05.728012",
     "exception": false,
     "start_time": "2021-08-01T07:48:00.854515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_df\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-sentence",
   "metadata": {
    "papermill": {
     "duration": 0.043098,
     "end_time": "2021-08-01T07:48:05.814130",
     "exception": false,
     "start_time": "2021-08-01T07:48:05.771032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "inside-review",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:05.914252Z",
     "iopub.status.busy": "2021-08-01T07:48:05.913293Z",
     "iopub.status.idle": "2021-08-01T07:48:05.915913Z",
     "shell.execute_reply": "2021-08-01T07:48:05.915479Z",
     "shell.execute_reply.started": "2021-07-26T12:01:53.31087Z"
    },
    "papermill": {
     "duration": 0.061001,
     "end_time": "2021-08-01T07:48:05.916031",
     "exception": false,
     "start_time": "2021-08-01T07:48:05.855030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abstract-lodging",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:06.019582Z",
     "iopub.status.busy": "2021-08-01T07:48:06.018666Z",
     "iopub.status.idle": "2021-08-01T07:48:06.021637Z",
     "shell.execute_reply": "2021-08-01T07:48:06.021125Z",
     "shell.execute_reply.started": "2021-07-26T12:01:53.326586Z"
    },
    "papermill": {
     "duration": 0.064397,
     "end_time": "2021-08-01T07:48:06.021761",
     "exception": false,
     "start_time": "2021-08-01T07:48:05.957364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        \n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "            \n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adjustable-accent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:06.129278Z",
     "iopub.status.busy": "2021-08-01T07:48:06.128321Z",
     "iopub.status.idle": "2021-08-01T07:48:06.131078Z",
     "shell.execute_reply": "2021-08-01T07:48:06.130641Z",
     "shell.execute_reply.started": "2021-07-26T12:01:53.344634Z"
    },
    "papermill": {
     "duration": 0.067654,
     "end_time": "2021-08-01T07:48:06.131202",
     "exception": false,
     "start_time": "2021-08-01T07:48:06.063548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "permanent-rebate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:06.230856Z",
     "iopub.status.busy": "2021-08-01T07:48:06.229870Z",
     "iopub.status.idle": "2021-08-01T07:48:06.232766Z",
     "shell.execute_reply": "2021-08-01T07:48:06.232333Z",
     "shell.execute_reply.started": "2021-07-26T12:01:53.35801Z"
    },
    "papermill": {
     "duration": 0.060364,
     "end_time": "2021-08-01T07:48:06.232916",
     "exception": false,
     "start_time": "2021-08-01T07:48:06.172552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids']\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                preds += logits\n",
    "        return preds\n",
    "\n",
    "def config(fold, model_name, load_model_path):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    \n",
    "    max_len = 250\n",
    "    batch_size = 8\n",
    "\n",
    "    model, tokenizer = make_model(\n",
    "        model_name=model_name, \n",
    "        num_labels=1\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    \n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-metabolism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-25T09:11:47.35772Z",
     "iopub.status.busy": "2021-07-25T09:11:47.357351Z",
     "iopub.status.idle": "2021-07-25T09:11:47.362915Z",
     "shell.execute_reply": "2021-07-25T09:11:47.361981Z",
     "shell.execute_reply.started": "2021-07-25T09:11:47.357634Z"
    },
    "papermill": {
     "duration": 0.043852,
     "end_time": "2021-08-01T07:48:06.318891",
     "exception": false,
     "start_time": "2021-08-01T07:48:06.275039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "after-electricity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:06.412563Z",
     "iopub.status.busy": "2021-08-01T07:48:06.411488Z",
     "iopub.status.idle": "2021-08-01T07:48:06.414561Z",
     "shell.execute_reply": "2021-08-01T07:48:06.414073Z",
     "shell.execute_reply.started": "2021-07-26T12:01:53.373955Z"
    },
    "papermill": {
     "duration": 0.053941,
     "end_time": "2021-08-01T07:48:06.414694",
     "exception": false,
     "start_time": "2021-08-01T07:48:06.360753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(fold=0, model_name=None, load_model_path=None):\n",
    "    model, tokenizer, \\\n",
    "        test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    \n",
    "    import time\n",
    "    wtf = \"Variations of the delayed-write policy differ in when modified data blocks\\\n",
    "    are flushed to the server. One alternative is to flush a block when it is about to\\\n",
    "    be ejected from the client’s cache. This option can result in good performance,\\\n",
    "    but some blocks can reside in the client’s cache a long time before they are\\\n",
    "    written back to the server. A compromise between this alternative and the\\\n",
    "    write-through policy is to scan the cache at regular intervals and to flush\\\n",
    "    blocks that have been modified since the most recent scan, just as UNIX scans\\\n",
    "    its local cache. Sprite uses this policy with a 30-second interval. NFS uses the\\\n",
    "    policy for file data, but once a write is issued to the server during a cache\\\n",
    "    flush, the write must reach the server ’s disk before it is considered complete.\\\n",
    "    NFS treats metadata (directory data and file-attribute data) differently. Any\\\n",
    "    metadata changes are issued synchronously to the server. Thus, file-structure\\\n",
    "    loss and directory-structure corruption are avoided when a client or the server\\\n",
    "    crashes.\"\n",
    "\n",
    "    evaluator = Evaluator(model, scaler)\n",
    "\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "\n",
    "    preds = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "helpful-installation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:48:06.506571Z",
     "iopub.status.busy": "2021-08-01T07:48:06.505919Z",
     "iopub.status.idle": "2021-08-01T07:50:48.189532Z",
     "shell.execute_reply": "2021-08-01T07:50:48.190195Z",
     "shell.execute_reply.started": "2021-07-26T12:01:53.385461Z"
    },
    "papermill": {
     "duration": 161.733438,
     "end_time": "2021-08-01T07:50:48.190461",
     "exception": false,
     "start_time": "2021-08-01T07:48:06.457023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:42<02:50, 42.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:12<01:44, 34.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:42<01:05, 32.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:12<00:31, 31.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:41<00:00, 32.34s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_df1 = pd.DataFrame()\n",
    "#pred_df2 = pd.DataFrame()\n",
    "pred_df3 = pd.DataFrame()\n",
    "\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df1[f'fold{fold}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-base/', '../input/commonlit-roberta-base-i/')\n",
    "    #pred_df2[f'fold{fold+5}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/roberta-large-itptfit/')\n",
    "    pred_df3[f'fold{fold+10}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "greatest-julian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:50:48.293647Z",
     "iopub.status.busy": "2021-08-01T07:50:48.291775Z",
     "iopub.status.idle": "2021-08-01T07:50:48.294401Z",
     "shell.execute_reply": "2021-08-01T07:50:48.294897Z",
     "shell.execute_reply.started": "2021-07-26T12:04:47.244113Z"
    },
    "papermill": {
     "duration": 0.058315,
     "end_time": "2021-08-01T07:50:48.295056",
     "exception": false,
     "start_time": "2021-08-01T07:50:48.236741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df1 = np.array(pred_df1)\n",
    "#pred_df2 = np.array(pred_df2)\n",
    "pred_df3 = np.array(pred_df3)\n",
    "\n",
    "model2_predictions = (pred_df1.mean(axis=1) * 0.6) + (pred_df3.mean(axis=1) * 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-backing",
   "metadata": {
    "papermill": {
     "duration": 0.048003,
     "end_time": "2021-08-01T07:50:48.389660",
     "exception": false,
     "start_time": "2021-08-01T07:50:48.341657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 3 \n",
    "\n",
    "Inspired from: https://www.kaggle.com/jcesquiveld/best-transformer-representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "laden-bicycle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:50:48.490734Z",
     "iopub.status.busy": "2021-08-01T07:50:48.490084Z",
     "iopub.status.idle": "2021-08-01T07:50:48.626879Z",
     "shell.execute_reply": "2021-08-01T07:50:48.626293Z",
     "shell.execute_reply.started": "2021-07-26T12:04:47.2524Z"
    },
    "papermill": {
     "duration": 0.190266,
     "end_time": "2021-08-01T07:50:48.627014",
     "exception": false,
     "start_time": "2021-08-01T07:50:48.436748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gc; gc.enable()\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sweet-heritage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:50:48.728607Z",
     "iopub.status.busy": "2021-08-01T07:50:48.728061Z",
     "iopub.status.idle": "2021-08-01T07:50:48.738456Z",
     "shell.execute_reply": "2021-08-01T07:50:48.737885Z",
     "shell.execute_reply.started": "2021-07-26T12:04:47.404285Z"
    },
    "papermill": {
     "duration": 0.0669,
     "end_time": "2021-08-01T07:50:48.738583",
     "exception": false,
     "start_time": "2021-08-01T07:50:48.671683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input/commonlitreadabilityprize'\n",
    "MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n",
    "CHECKPOINT_DIR = '../input/clrp-mean-pooling/'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAX_LENGTH = 248\n",
    "TEST_BATCH_SIZE = 1\n",
    "HIDDEN_SIZE = 1024\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "SEEDS = [113]\n",
    "\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intended-ending",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:50:48.844825Z",
     "iopub.status.busy": "2021-08-01T07:50:48.843153Z",
     "iopub.status.idle": "2021-08-01T07:50:48.845542Z",
     "shell.execute_reply": "2021-08-01T07:50:48.845982Z",
     "shell.execute_reply.started": "2021-07-26T12:04:47.421976Z"
    },
    "papermill": {
     "duration": 0.062213,
     "end_time": "2021-08-01T07:50:48.846127",
     "exception": false,
     "start_time": "2021-08-01T07:50:48.783914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPoolingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        logits = self.linear(mean_embeddings)\n",
    "        \n",
    "        preds = logits.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n",
    "            return loss\n",
    "        else:\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vulnerable-filing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:50:48.948683Z",
     "iopub.status.busy": "2021-08-01T07:50:48.947756Z",
     "iopub.status.idle": "2021-08-01T07:50:49.148930Z",
     "shell.execute_reply": "2021-08-01T07:50:49.148355Z",
     "shell.execute_reply.started": "2021-07-26T12:04:47.439144Z"
    },
    "papermill": {
     "duration": 0.256324,
     "end_time": "2021-08-01T07:50:49.149074",
     "exception": false,
     "start_time": "2021-08-01T07:50:48.892750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_loader(data):\n",
    "\n",
    "    x_test = data.excerpt.tolist()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    encoded_test = tokenizer.batch_encode_plus(\n",
    "        x_test, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    dataset_test = TensorDataset(\n",
    "        encoded_test['input_ids'],\n",
    "        encoded_test['attention_mask']\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        sampler = SequentialSampler(dataset_test),\n",
    "        batch_size=TEST_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return dataloader_test\n",
    "\n",
    "test_dataloader = get_test_loader(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "subtle-laugh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:50:49.275919Z",
     "iopub.status.busy": "2021-08-01T07:50:49.275023Z",
     "iopub.status.idle": "2021-08-01T07:52:34.898031Z",
     "shell.execute_reply": "2021-08-01T07:52:34.897549Z",
     "shell.execute_reply.started": "2021-07-26T12:04:47.629487Z"
    },
    "papermill": {
     "duration": 105.704283,
     "end_time": "2021-08-01T07:52:34.898163",
     "exception": false,
     "start_time": "2021-08-01T07:50:49.193880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984237523811438cb8a466b110563f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_114_1.pth\n",
      "\n",
      "Using model_114_2.pth\n",
      "\n",
      "Using model_114_3.pth\n",
      "\n",
      "Using model_114_4.pth\n",
      "\n",
      "Using model_114_5.pth\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "for seed in SEEDS:\n",
    "    \n",
    "    fold_predictions = []\n",
    "    \n",
    "    for fold in tqdm(range(NUM_FOLDS)):\n",
    "        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "        \n",
    "        print(f\"\\nUsing {model_path}\")\n",
    "        \n",
    "        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "        model = MeanPoolingModel(MODEL_DIR)\n",
    "        model.load_state_dict(torch.load(model_path)) \n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         None,\n",
    "                     }\n",
    "\n",
    "     \n",
    "            preds = model(**inputs).item()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "        del model \n",
    "        gc.collect()\n",
    "            \n",
    "        fold_predictions.append(predictions)\n",
    "    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n",
    "    \n",
    "model3_predictions = np.mean(all_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "intense-lounge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:52:35.015343Z",
     "iopub.status.busy": "2021-08-01T07:52:35.014680Z",
     "iopub.status.idle": "2021-08-01T07:52:35.022567Z",
     "shell.execute_reply": "2021-08-01T07:52:35.022134Z",
     "shell.execute_reply.started": "2021-08-01T06:32:20.986882Z"
    },
    "papermill": {
     "duration": 0.078998,
     "end_time": "2021-08-01T07:52:35.022692",
     "exception": false,
     "start_time": "2021-08-01T07:52:34.943694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, Sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def convert_examples_to_features(text, tokenizer, max_len):\n",
    "\n",
    "    tok = tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    return tok\n",
    "\n",
    "class CLRPDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.tolist()\n",
    "        if not is_test:\n",
    "            self.targets = self.data.target.tolist()\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt = self.excerpts[item]\n",
    "            label = self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.float),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.head = AttentionHead(self.h_size*4)\n",
    "        self.linear = nn.Linear(self.h_size*2, 1)\n",
    "        self.linear_out = nn.Linear(self.h_size*8, 1)\n",
    "\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "       \n",
    "        all_hidden_states = torch.stack(transformer_out.hidden_states)\n",
    "        cat_over_last_layers = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "        \n",
    "        cls_pooling = cat_over_last_layers[:, 0]   \n",
    "        head_logits = self.head(cat_over_last_layers)\n",
    "        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "def predic(data, in_folder_path):\n",
    "    in_folder_path = Path(in_folder_path)\n",
    "    mp = \"Protocol stacks, as specified by network layering models, add information\\\n",
    "    to a message to ensure that it reaches its destination. A naming system (such\\\n",
    "    as DNS) must be used to translate from a host name to a network address, and\\\n",
    "    another protocol (such as ARP) may be needed to translate the network number\\\n",
    "    to a network device address (an Ethernet address, for instance). If systems are\\\n",
    "    located on separate networks, routers are needed to pass packets from source\\\n",
    "    network to destination network.\\\n",
    "    There are many challenges to overcome for a distributed system to work\\\n",
    "    correctly. Issues include naming of nodes and processes in the system, fault\\\n",
    "    tolerance, error recovery, and scalability.\\\n",
    "    A DFS is a file-service system whose clients, servers, and storage devices\\\n",
    "    are dispersed among the sites of a distributed system. Accordingly, service\\\n",
    "    activity has to be carried out across the network; instead of a single centralized\\\n",
    "    data repository, there are multiple independent storage devices.\\\n",
    "    Ideally, a DFS should look to its clients like a conventional, centralized\\\n",
    "    file system. The multiplicity and dispersion of its servers and storage devices\\\n",
    "    should be transparent. A transparent DFS facilitates client mobility by bringing\\\n",
    "    the client’s environment to the site where the client logs in.\\\n",
    "    There are several approaches to naming schemes in a DFS. In the simplest\\\n",
    "    approach, files are named by some combination of their host name and local\\\n",
    "    name, which guarantees a unique system-wide name. Another approach,\\\n",
    "    popularized by NFS, provides a means to attach remote directories to local\\\n",
    "    directories, thus giving the appearance of a coherent directory tree.\\\n",
    "    Requests to access a remote file are usually handled by two complementary\\\n",
    "    methods. With remote service, requests for accesses are delivered to the server.\\\n",
    "    The server machine performs the accesses, and the results are forwarded back\\\n",
    "    to the client. With caching, if the data needed to satisfy the access request are\\\n",
    "    not already cached, then a copy of the data is brought from the server to the\\\n",
    "    client. Accesses are performed on the cached copy. The problem of keeping the\\\n",
    "    cached copies consistent with the master file is the cache-consistency problem.\\\n",
    "    Practice Exercises\\\n",
    "    17.1 Why would it be a bad idea for gateways to pass broadcast packets\\\n",
    "    between networks? What would be the advantages of doing so?\\\n",
    "    17.2 Discuss the advantages and disadvantages of caching name transla-\\\n",
    "        tions for computers located in remote domains.\\\n",
    "    17.3 What are the advantages and disadvantages of using circuit switching?\\\n",
    "    For what kinds of applications is circuit switching a viable strategy?\\\n",
    "    17.4 What are two formidable problems that designers must solve to\\\n",
    "    implement a network system that has the quality of transparency?\"\n",
    "\n",
    "    \n",
    "    models_folder_path = Path(in_folder_path / 'models')\n",
    "    models_preds = []\n",
    "    \n",
    "    for model_num in range(5):\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(in_folder_path)\n",
    "            \n",
    "        print(f'Inference#{model_num+1}/5')\n",
    "        test_ds = CLRPDataset(data=data, tokenizer=tokenizer, max_len=256, is_test=True)\n",
    "        test_sampler = SequentialSampler(test_ds)\n",
    "        test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=4)\n",
    "        model = torch.load(models_folder_path / f'best_model_{model_num}.pt').to('cuda')\n",
    "        print(f'Model {models_folder_path} / best_model_{model_num}.pt is pushed to Device')\n",
    "\n",
    "        all_preds = []\n",
    "        model.eval()\n",
    "\n",
    "        for step,batch in enumerate(test_dataloader):\n",
    "            sent_id, mask = batch['input_ids'].to('cuda'), batch['attention_mask'].to('cuda')\n",
    "            with torch.no_grad():\n",
    "                preds = model(sent_id, mask)\n",
    "                all_preds += preds.flatten().cpu().tolist()\n",
    "\n",
    "        models_preds.append(all_preds)\n",
    "        del model, tokenizer, test_dataloader, test_sampler, test_ds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.array(models_preds).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "proof-albuquerque",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:52:35.120608Z",
     "iopub.status.busy": "2021-08-01T07:52:35.119910Z",
     "iopub.status.idle": "2021-08-01T07:53:45.281025Z",
     "shell.execute_reply": "2021-08-01T07:53:45.280532Z",
     "shell.execute_reply.started": "2021-07-26T12:07:47.92116Z"
    },
    "papermill": {
     "duration": 70.211573,
     "end_time": "2021-08-01T07:53:45.281160",
     "exception": false,
     "start_time": "2021-08-01T07:52:35.069587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference#1/5\n",
      "Model ../input/comdbli/models / best_model_0.pt is pushed to Device\n",
      "Inference#2/5\n",
      "Model ../input/comdbli/models / best_model_1.pt is pushed to Device\n",
      "Inference#3/5\n",
      "Model ../input/comdbli/models / best_model_2.pt is pushed to Device\n",
      "Inference#4/5\n",
      "Model ../input/comdbli/models / best_model_3.pt is pushed to Device\n",
      "Inference#5/5\n",
      "Model ../input/comdbli/models / best_model_4.pt is pushed to Device\n"
     ]
    }
   ],
   "source": [
    "pd1 = predic(test_df, '../input/comdbli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "settled-litigation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:53:45.385358Z",
     "iopub.status.busy": "2021-08-01T07:53:45.384130Z",
     "iopub.status.idle": "2021-08-01T07:53:45.389191Z",
     "shell.execute_reply": "2021-08-01T07:53:45.388678Z",
     "shell.execute_reply.started": "2021-07-26T12:09:37.065477Z"
    },
    "papermill": {
     "duration": 0.059315,
     "end_time": "2021-08-01T07:53:45.389333",
     "exception": false,
     "start_time": "2021-08-01T07:53:45.330018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29079663, -0.46820574, -0.38955538, -2.14445047, -1.80839045,\n",
       "       -1.17013342,  0.25433525])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pleasant-binding",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:53:45.506197Z",
     "iopub.status.busy": "2021-08-01T07:53:45.504540Z",
     "iopub.status.idle": "2021-08-01T07:53:45.506969Z",
     "shell.execute_reply": "2021-08-01T07:53:45.507372Z",
     "shell.execute_reply.started": "2021-08-01T06:32:24.159972Z"
    },
    "papermill": {
     "duration": 0.065717,
     "end_time": "2021-08-01T07:53:45.507520",
     "exception": false,
     "start_time": "2021-08-01T07:53:45.441803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.head = AttentionHead(self.h_size*4)\n",
    "        self.linear = nn.Linear(self.h_size*8, self.h_size // 2)\n",
    "        self.linear_out = nn.Linear(self.h_size // 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "       \n",
    "        all_hidden_states = torch.stack(transformer_out.hidden_states)\n",
    "        cat_over_last_layers = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "        \n",
    "        cls_pooling = cat_over_last_layers[:, 0]   \n",
    "        head_logits = self.head(cat_over_last_layers)\n",
    "        logits = self.tanh(self.linear(torch.cat([head_logits, cls_pooling], -1)))\n",
    "        y_hat = self.linear_out(logits)\n",
    "        \n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "steady-meditation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:53:45.609139Z",
     "iopub.status.busy": "2021-08-01T07:53:45.608555Z",
     "iopub.status.idle": "2021-08-01T07:55:55.904492Z",
     "shell.execute_reply": "2021-08-01T07:55:55.903994Z",
     "shell.execute_reply.started": "2021-07-28T13:26:40.694199Z"
    },
    "papermill": {
     "duration": 130.348409,
     "end_time": "2021-08-01T07:55:55.904628",
     "exception": false,
     "start_time": "2021-08-01T07:53:45.556219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference#1/5\n",
      "Model ../input/dbx777/models / best_model_0.pt is pushed to Device\n",
      "Inference#2/5\n",
      "Model ../input/dbx777/models / best_model_1.pt is pushed to Device\n",
      "Inference#3/5\n",
      "Model ../input/dbx777/models / best_model_2.pt is pushed to Device\n",
      "Inference#4/5\n",
      "Model ../input/dbx777/models / best_model_3.pt is pushed to Device\n",
      "Inference#5/5\n",
      "Model ../input/dbx777/models / best_model_4.pt is pushed to Device\n"
     ]
    }
   ],
   "source": [
    "pd3 = predic(test_df, '../input/dbx777')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "usual-seafood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:55:56.012517Z",
     "iopub.status.busy": "2021-08-01T07:55:56.011842Z",
     "iopub.status.idle": "2021-08-01T07:55:56.019009Z",
     "shell.execute_reply": "2021-08-01T07:55:56.018496Z",
     "shell.execute_reply.started": "2021-07-28T13:29:15.494464Z"
    },
    "papermill": {
     "duration": 0.062874,
     "end_time": "2021-08-01T07:55:56.019148",
     "exception": false,
     "start_time": "2021-08-01T07:55:55.956274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29928167, -0.33505895, -0.41167788, -2.06349804, -1.85605731,\n",
       "       -1.22650595,  0.26501839])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "early-technician",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:55:56.151229Z",
     "iopub.status.busy": "2021-08-01T07:55:56.150522Z",
     "iopub.status.idle": "2021-08-01T07:56:57.072888Z",
     "shell.execute_reply": "2021-08-01T07:56:57.072313Z",
     "shell.execute_reply.started": "2021-08-01T06:33:10.341178Z"
    },
    "papermill": {
     "duration": 60.995575,
     "end_time": "2021-08-01T07:56:57.073050",
     "exception": false,
     "start_time": "2021-08-01T07:55:56.077475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference#1/5\n",
      "Model ../input/notebook6921fb919e/models / best_model_0.pt is pushed to Device\n",
      "Inference#2/5\n",
      "Model ../input/notebook6921fb919e/models / best_model_1.pt is pushed to Device\n",
      "Inference#3/5\n",
      "Model ../input/notebook6921fb919e/models / best_model_2.pt is pushed to Device\n",
      "Inference#4/5\n",
      "Model ../input/notebook6921fb919e/models / best_model_3.pt is pushed to Device\n",
      "Inference#5/5\n",
      "Model ../input/notebook6921fb919e/models / best_model_4.pt is pushed to Device\n"
     ]
    }
   ],
   "source": [
    "models_folder_path = Path('../input/notebook6921fb919e/models')\n",
    "models_preds = []\n",
    "    \n",
    "for model_num in range(5):\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained('../input/comdbli')\n",
    "    data = test_df\n",
    "    print(f'Inference#{model_num+1}/5')\n",
    "    test_ds = CLRPDataset(data=data, tokenizer=tokenizer, max_len=256, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_ds)\n",
    "    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=4)\n",
    "    model = torch.load(models_folder_path / f'best_model_{model_num}.pt').to('cuda')\n",
    "    print(f'Model {models_folder_path} / best_model_{model_num}.pt is pushed to Device')\n",
    "\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "\n",
    "    for step,batch in enumerate(test_dataloader):\n",
    "        sent_id, mask = batch['input_ids'].to('cuda'), batch['attention_mask'].to('cuda')\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            all_preds += preds.flatten().cpu().tolist()\n",
    "\n",
    "    models_preds.append(all_preds)\n",
    "    del model, tokenizer, test_dataloader, test_sampler, test_ds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "pd4 = np.array(models_preds).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "straight-editor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:56:57.204247Z",
     "iopub.status.busy": "2021-08-01T07:56:57.203491Z",
     "iopub.status.idle": "2021-08-01T07:56:57.210700Z",
     "shell.execute_reply": "2021-08-01T07:56:57.210180Z",
     "shell.execute_reply.started": "2021-08-01T06:34:31.682795Z"
    },
    "papermill": {
     "duration": 0.076155,
     "end_time": "2021-08-01T07:56:57.210856",
     "exception": false,
     "start_time": "2021-08-01T07:56:57.134701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23531469, -0.44356807, -0.47543548, -2.14265709, -1.72869146,\n",
       "       -1.15228729,  0.35644643])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "hundred-initial",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:56:57.362007Z",
     "iopub.status.busy": "2021-08-01T07:56:57.359741Z",
     "iopub.status.idle": "2021-08-01T07:56:57.362842Z",
     "shell.execute_reply": "2021-08-01T07:56:57.363315Z",
     "shell.execute_reply.started": "2021-07-26T12:13:27.072428Z"
    },
    "papermill": {
     "duration": 0.080626,
     "end_time": "2021-08-01T07:56:57.363514",
     "exception": false,
     "start_time": "2021-08-01T07:56:57.282888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.head = AttentionHead(self.h_size*4)\n",
    "        self.linear = nn.Linear(self.h_size*2, 1)\n",
    "        self.linear_out = nn.Linear(self.h_size*8, 1)\n",
    "\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "       \n",
    "        all_hidden_states = torch.stack(transformer_out.hidden_states)\n",
    "        cat_over_last_layers = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "        \n",
    "        cls_pooling = cat_over_last_layers[:, 0]   \n",
    "        head_logits = self.head(cat_over_last_layers)\n",
    "        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n",
    "        \n",
    "        return y_hat\n",
    "def predic2(data, in_folder_path):\n",
    "    in_folder_path = Path(in_folder_path)\n",
    "\n",
    "    tp = \"In the final part of the book, we integrate the concepts described earlier\\\n",
    "    by examining real operating systems. We cover two such systems in\\\n",
    "    detail — Linux and Windows 7. We chose Linux for several reasons: it is\\\n",
    "    popular, it is freely available, and it represents a full-featured UNIX system.\\\n",
    "    This gives a student of operating systems an opportunity to read — and\\\n",
    "    modify — real operating-system source code.\\\n",
    "    We also cover Windows 7 in detail. This recent operating system from\\\n",
    "    Microsoft is gaining popularity not only in the standalone-machine market\\\n",
    "    but also in the workgroup – server market. We chose Windows 7 because\\\n",
    "    it provides an opportunity to study a modern operating system that has\\\n",
    "    a design and implementation drastically different from those of UNIX.\\\n",
    "    In addition, we briefly discuss other highly influential operating sys-\\\n",
    "    tems. Finally, we provide on-line coverage of two more systems: FreeBSD\\\n",
    "    and Mach. The FreeBSD system is another UNIX system. However,\\\n",
    "    whereas Linux combines features from several UNIX systems, FreeBSD\\\n",
    "    is based on the BSD model. FreeBSD source code, like Linux source\\\n",
    "    code, is freely available. Mach is a modern operating system that provides\\\n",
    "    compatibility with BSD UNIX.\"\n",
    "    \n",
    "    models_folder_path = Path(in_folder_path / 'models')\n",
    "    models_preds = []\n",
    "    \n",
    "    for model_num in range(5):\n",
    "        \n",
    "        tokenizer = torch.load('../input/tokenizers/roberta-tokenizer.pt')\n",
    "            \n",
    "        print(f'Inference#{model_num+1}/5')\n",
    "        test_ds = CLRPDataset(data=data, tokenizer=tokenizer, max_len=256, is_test=True)\n",
    "        test_sampler = SequentialSampler(test_ds)\n",
    "        test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=4)\n",
    "        model = torch.load(models_folder_path / f'best_model_{model_num}.pt').to('cuda')\n",
    "        print(f'Model {models_folder_path} / best_model_{model_num}.pt is pushed to Device')\n",
    "\n",
    "        all_preds = []\n",
    "        model.eval()\n",
    "\n",
    "        for step,batch in enumerate(test_dataloader):\n",
    "            sent_id, mask = batch['input_ids'].to('cuda'), batch['attention_mask'].to('cuda')\n",
    "            with torch.no_grad():\n",
    "                preds = model(sent_id, mask)\n",
    "                all_preds += preds.flatten().cpu().tolist()\n",
    "\n",
    "        models_preds.append(all_preds)\n",
    "        del model, tokenizer, test_dataloader, test_sampler, test_ds\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.array(models_preds).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "contained-cycling",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:56:57.494622Z",
     "iopub.status.busy": "2021-08-01T07:56:57.493810Z",
     "iopub.status.idle": "2021-08-01T07:57:58.782268Z",
     "shell.execute_reply": "2021-08-01T07:57:58.781682Z",
     "shell.execute_reply.started": "2021-07-26T12:13:29.223124Z"
    },
    "papermill": {
     "duration": 61.355238,
     "end_time": "2021-08-01T07:57:58.782425",
     "exception": false,
     "start_time": "2021-08-01T07:56:57.427187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference#1/5\n",
      "Model ../input/0463-robertalarge/models / best_model_0.pt is pushed to Device\n",
      "Inference#2/5\n",
      "Model ../input/0463-robertalarge/models / best_model_1.pt is pushed to Device\n",
      "Inference#3/5\n",
      "Model ../input/0463-robertalarge/models / best_model_2.pt is pushed to Device\n",
      "Inference#4/5\n",
      "Model ../input/0463-robertalarge/models / best_model_3.pt is pushed to Device\n",
      "Inference#5/5\n",
      "Model ../input/0463-robertalarge/models / best_model_4.pt is pushed to Device\n"
     ]
    }
   ],
   "source": [
    "pd2 = predic2(test_df, '../input/0463-robertalarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "inside-egyptian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:57:58.910542Z",
     "iopub.status.busy": "2021-08-01T07:57:58.909319Z",
     "iopub.status.idle": "2021-08-01T07:57:58.913901Z",
     "shell.execute_reply": "2021-08-01T07:57:58.914384Z",
     "shell.execute_reply.started": "2021-07-26T12:15:12.932492Z"
    },
    "papermill": {
     "duration": 0.070311,
     "end_time": "2021-08-01T07:57:58.914570",
     "exception": false,
     "start_time": "2021-08-01T07:57:58.844259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.34481332, -0.48203456, -0.48146576, -2.17766552, -1.91580055,\n",
       "       -1.30661547,  0.17578244])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "turkish-newman",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:57:59.052507Z",
     "iopub.status.busy": "2021-08-01T07:57:59.051213Z",
     "iopub.status.idle": "2021-08-01T07:57:59.055319Z",
     "shell.execute_reply": "2021-08-01T07:57:59.055821Z",
     "shell.execute_reply.started": "2021-07-26T12:15:27.102352Z"
    },
    "papermill": {
     "duration": 0.075145,
     "end_time": "2021-08-01T07:57:59.055979",
     "exception": false,
     "start_time": "2021-08-01T07:57:58.980834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37558921, -0.487815  , -0.45136422, -2.27267265, -1.81203254,\n",
       "       -1.20362545,  0.19535162])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = ((model1_predictions + mdp + pred_df1.mean(axis=1) + pred_df3.mean(axis=1) + model3_predictions)/5 + ((pd1 + pd4 + pd3)/3 + pd2)/2)/2\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fantastic-london",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:57:59.202208Z",
     "iopub.status.busy": "2021-08-01T07:57:59.201083Z",
     "iopub.status.idle": "2021-08-01T07:57:59.211808Z",
     "shell.execute_reply": "2021-08-01T07:57:59.211243Z",
     "shell.execute_reply.started": "2021-07-26T12:15:34.796757Z"
    },
    "papermill": {
     "duration": 0.086076,
     "end_time": "2021-08-01T07:57:59.211961",
     "exception": false,
     "start_time": "2021-08-01T07:57:59.125885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.437155</td>\n",
       "      <td>-0.466744</td>\n",
       "      <td>-0.403613</td>\n",
       "      <td>-0.375589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.656492</td>\n",
       "      <td>-0.601198</td>\n",
       "      <td>-0.390972</td>\n",
       "      <td>-0.487815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.354968</td>\n",
       "      <td>-0.472127</td>\n",
       "      <td>-0.432696</td>\n",
       "      <td>-0.451364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.515151</td>\n",
       "      <td>-2.403332</td>\n",
       "      <td>-2.249735</td>\n",
       "      <td>-2.272673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.678813</td>\n",
       "      <td>-1.791369</td>\n",
       "      <td>-1.838367</td>\n",
       "      <td>-1.812033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.427003</td>\n",
       "      <td>-1.100678</td>\n",
       "      <td>-1.113217</td>\n",
       "      <td>-1.203625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050763</td>\n",
       "      <td>0.160442</td>\n",
       "      <td>0.106686</td>\n",
       "      <td>0.195352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model1    model2    model3  ensemble\n",
       "0 -0.437155 -0.466744 -0.403613 -0.375589\n",
       "1 -0.656492 -0.601198 -0.390972 -0.487815\n",
       "2 -0.354968 -0.472127 -0.432696 -0.451364\n",
       "3 -2.515151 -2.403332 -2.249735 -2.272673\n",
       "4 -1.678813 -1.791369 -1.838367 -1.812033\n",
       "5 -1.427003 -1.100678 -1.113217 -1.203625\n",
       "6  0.050763  0.160442  0.106686  0.195352"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(np.vstack((model1_predictions, model2_predictions, model3_predictions, predictions)).transpose(), \n",
    "                       columns=['model1','model2','model3','ensemble'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "suburban-vatican",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:57:59.353911Z",
     "iopub.status.busy": "2021-08-01T07:57:59.352882Z",
     "iopub.status.idle": "2021-08-01T07:57:59.358491Z",
     "shell.execute_reply": "2021-08-01T07:57:59.357988Z"
    },
    "papermill": {
     "duration": 0.078385,
     "end_time": "2021-08-01T07:57:59.358625",
     "exception": false,
     "start_time": "2021-08-01T07:57:59.280240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.375589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.487815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.451364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.272673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.812033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.203625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.195352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.375589\n",
       "1  f0953f0a5 -0.487815\n",
       "2  0df072751 -0.451364\n",
       "3  04caf4e0c -2.272673\n",
       "4  0e63f8bea -1.812033\n",
       "5  12537fe78 -1.203625\n",
       "6  965e592c0  0.195352"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.target = predictions\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "painful-transfer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T07:57:59.493911Z",
     "iopub.status.busy": "2021-08-01T07:57:59.493252Z",
     "iopub.status.idle": "2021-08-01T07:57:59.826088Z",
     "shell.execute_reply": "2021-08-01T07:57:59.825489Z"
    },
    "papermill": {
     "duration": 0.399089,
     "end_time": "2021-08-01T07:57:59.826234",
     "exception": false,
     "start_time": "2021-08-01T07:57:59.427145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 703.980963,
   "end_time": "2021-08-01T07:58:02.936674",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-01T07:46:18.955711",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03501afb7b6544de9566fdd186094b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0d6605eaa704654b3b8d1cf24d18556",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_90171d4669e5485cbdd4a45bc0685419",
       "value": 5.0
      }
     },
     "177d08691da24c5f9f478cec8dde8b41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "28815061ae59479d9e8446f4717731ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "494a583f8e3c40aea53e9f58528ee16d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5552da0299e74055be0949eecff333af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_79c439dfb6ae49e285bc8178b86e1bb0",
       "placeholder": "​",
       "style": "IPY_MODEL_177d08691da24c5f9f478cec8dde8b41",
       "value": " 5/5 [01:45&lt;00:00, 21.05s/it]"
      }
     },
     "79c439dfb6ae49e285bc8178b86e1bb0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90171d4669e5485cbdd4a45bc0685419": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "984237523811438cb8a466b110563f3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a864500d29514d638335826b21d297a1",
        "IPY_MODEL_03501afb7b6544de9566fdd186094b42",
        "IPY_MODEL_5552da0299e74055be0949eecff333af"
       ],
       "layout": "IPY_MODEL_494a583f8e3c40aea53e9f58528ee16d"
      }
     },
     "a864500d29514d638335826b21d297a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cd740f833b134756b5404f55fc338a68",
       "placeholder": "​",
       "style": "IPY_MODEL_28815061ae59479d9e8446f4717731ce",
       "value": "100%"
      }
     },
     "b0d6605eaa704654b3b8d1cf24d18556": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd740f833b134756b5404f55fc338a68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
